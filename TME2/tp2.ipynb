{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 20:34:12.911354: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-13 20:34:12.959770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-13 20:34:12.961606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-13 20:34:13.847043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) =tf.keras.datasets.boston_housing.load_data(\n",
    "    path=\"boston_housing.npz\", test_split=0.2, seed=1234\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIM     per capita crime rate by town\n",
    "\n",
    " ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    " INDUS    proportion of non-retail business acres per town\n",
    "\n",
    " CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "\n",
    " NOX      nitric oxides concentration (parts per 10 million)\n",
    "\n",
    " RM       average number of rooms per dwelling\n",
    "\n",
    " AGE      proportion of owner-occupied units built prior to 1940\n",
    "\n",
    " DIS      weighted distances to five Boston employment centres\n",
    "\n",
    " RAD      index of accessibility to radial highways\n",
    "\n",
    " TAX      full-value property-tax rate per $10,000\n",
    "\n",
    " PTRATIO  pupil-teacher ratio by town\n",
    "\n",
    " B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    " LSTAT    % lower status of the population\n",
    " \n",
    " MEDV     Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404,)\n",
      "(102, 13)\n",
      "(102,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.742918</td>\n",
       "      <td>11.877475</td>\n",
       "      <td>11.200569</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.556849</td>\n",
       "      <td>6.299928</td>\n",
       "      <td>69.308168</td>\n",
       "      <td>3.788781</td>\n",
       "      <td>9.626238</td>\n",
       "      <td>410.185644</td>\n",
       "      <td>18.460149</td>\n",
       "      <td>355.311782</td>\n",
       "      <td>12.777054</td>\n",
       "      <td>22.426733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.385410</td>\n",
       "      <td>23.950926</td>\n",
       "      <td>6.908259</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>0.116526</td>\n",
       "      <td>0.702557</td>\n",
       "      <td>28.004850</td>\n",
       "      <td>2.152334</td>\n",
       "      <td>8.751116</td>\n",
       "      <td>169.116234</td>\n",
       "      <td>2.180405</td>\n",
       "      <td>93.327775</td>\n",
       "      <td>7.156010</td>\n",
       "      <td>9.174751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.863000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.174200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.886500</td>\n",
       "      <td>45.550000</td>\n",
       "      <td>2.096725</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.300000</td>\n",
       "      <td>7.177500</td>\n",
       "      <td>16.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.795000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.211500</td>\n",
       "      <td>79.450000</td>\n",
       "      <td>3.100900</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.305000</td>\n",
       "      <td>11.490000</td>\n",
       "      <td>21.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.681942</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.626000</td>\n",
       "      <td>94.175000</td>\n",
       "      <td>5.035675</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.080000</td>\n",
       "      <td>17.102500</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>73.534100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>36.980000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.742918   11.877475   11.200569    0.069307    0.556849    6.299928   \n",
       "std      8.385410   23.950926    6.908259    0.254290    0.116526    0.702557   \n",
       "min      0.006320    0.000000    0.740000    0.000000    0.385000    3.863000   \n",
       "25%      0.082125    0.000000    5.190000    0.000000    0.453000    5.886500   \n",
       "50%      0.260420    0.000000    9.795000    0.000000    0.538000    6.211500   \n",
       "75%      3.681942   12.500000   18.100000    0.000000    0.631000    6.626000   \n",
       "max     73.534100  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.308168    3.788781    9.626238  410.185644   18.460149  355.311782   \n",
       "std     28.004850    2.152334    8.751116  169.116234    2.180405   93.327775   \n",
       "min      6.000000    1.174200    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.550000    2.096725    4.000000  279.000000   17.400000  375.300000   \n",
       "50%     79.450000    3.100900    5.000000  335.000000   19.100000  391.305000   \n",
       "75%     94.175000    5.035675   24.000000  666.000000   20.200000  396.080000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  404.000000  404.000000  \n",
       "mean    12.777054   22.426733  \n",
       "std      7.156010    9.174751  \n",
       "min      1.730000    5.000000  \n",
       "25%      7.177500   16.575000  \n",
       "50%     11.490000   21.150000  \n",
       "75%     17.102500   25.000000  \n",
       "max     36.980000   50.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = np.hstack((x_train, y_train.reshape(-1,1)))\n",
    "data = pd.DataFrame(data, columns=column_names)\n",
    "data.head()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dat(Dataset):\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tsuper(Dat, self).__init__()\n",
    "\t\tself.labels = torch.from_numpy(y).double()\n",
    "\t\tself.data = torch.from_numpy(x).double()\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.data[index], self.labels[index]\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train /= y_train.max()\n",
    "x_train = normalize(x_train, axis=0)\n",
    "x_test = normalize(x_test, axis=0)\n",
    "y_test /= y_test.max()\n",
    "data_test = Dat(x_test, y_test)\n",
    "x_test2 = data_test[0][0]\n",
    "y_test2 = data_test[1][0]\n",
    "train_dataset = Dat(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1000 [00:00<?, ?it/s]/home/pidoux/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/pidoux/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/pidoux/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_3685/536796875.py:32: RuntimeWarning: Mean of empty slice.\n",
      "  print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean_test).mean())\n",
      "/home/pidoux/.local/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  1%|‚ñà                                                                                | 13/1000 [00:01<00:56, 17.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  train MSE:  nan val MSE:  0.013653188995687546\n",
      "epoch 10  train MSE:  nan val MSE:  0.021326172316493\n",
      "epoch 20  train MSE:  nan val MSE:  0.02983027694441851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñà‚ñà‚ñà‚ñå                                                                             | 44/1000 [00:01<00:16, 56.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30  train MSE:  nan val MSE:  0.038789799623010005\n",
      "epoch 40  train MSE:  nan val MSE:  0.047734227211606334\n",
      "epoch 50  train MSE:  nan val MSE:  0.05655666574906647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                           | 74/1000 [00:01<00:11, 78.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60  train MSE:  nan val MSE:  0.06495340622812748\n",
      "epoch 70  train MSE:  nan val MSE:  0.07299981073835049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                         | 94/1000 [00:01<00:10, 84.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80  train MSE:  nan val MSE:  0.08067489079872593\n",
      "epoch 90  train MSE:  nan val MSE:  0.08794543735789859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                       | 104/1000 [00:01<00:10, 84.60it/s]"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(comment='mini-batch-size-40')\n",
    "model = torch.nn.Linear(13, 1).double()\n",
    "lossfn = torch.nn.MSELoss()\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n",
    "learningRate = 0.0005\n",
    "epochs = 1000\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    loss_mean = [] # pour calculer ensuite la moyenne des loss sur tous les batches\n",
    "    loss_mean_test = [] # pour calculer ensuite la moyenne des loss sur le test\n",
    "\n",
    "    for x_batch, y_batch in trainloader: # batch\n",
    "        # forward\n",
    "        y_pred = model(x_batch)\n",
    "        loss = lossfn(y_pred, y_batch)\n",
    "        writer.add_scalar(\"Loss/train\", loss, e)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #descente de gradient\n",
    "        with torch.no_grad():\n",
    "            model.weight -= learningRate * model.weight.grad\n",
    "            model.bias -= learningRate * model.bias.grad\n",
    "        model.weight.grad.zero_() #remise a zero des gradients\n",
    "        model.bias.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(x_test2)\n",
    "        loss_test = lossfn(y_pred_test, y_test2)\n",
    "        loss_mean_test.append(loss_test.item())\n",
    "        writer.add_scalar(\"Loss/test\", loss_test, e)\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean_test).mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='batch')\n",
    "model = torch.nn.Linear(13, 1).double()\n",
    "lossfn = torch.nn.MSELoss()\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=len(x_train), shuffle=True)\n",
    "learningRate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    loss_mean = [] # pour calculer ensuite la moyenne des loss sur tous les batches\n",
    "    loss_mean_test = [] # pour calculer ensuite la moyenne des loss sur le test\n",
    "\n",
    "    for x_batch, y_batch in trainloader: # batch\n",
    "        # forward\n",
    "        mult = model(x_batch)\n",
    "        loss = lossfn(mult, y_batch)\n",
    "        loss_mean.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train\", loss, e)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #descente de gradient\n",
    "        with torch.no_grad():\n",
    "            model.weight -= learningRate * model.weight.grad\n",
    "            model.bias -= learningRate * model.bias.grad\n",
    "        model.weight.grad.zero_() #remise a zero des gradients\n",
    "        model.bias.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        mult_test = model(x_test2)\n",
    "        loss_test = lossfn(mult_test, y_test2)\n",
    "        loss_mean_test.append(loss_test.item())\n",
    "        writer.add_scalar(\"Loss/test\", loss_test, e)\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean_test).mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='stochastique')\n",
    "model = torch.nn.Linear(13, 1).double()\n",
    "lossfn = torch.nn.MSELoss()\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "learningRate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    loss_mean = [] # pour calculer ensuite la moyenne des loss sur tous les batches\n",
    "    loss_mean_test = [] # pour calculer ensuite la moyenne des loss sur le test\n",
    "\n",
    "    for x_batch, y_batch in trainloader: # batch\n",
    "        # forward\n",
    "        mult = model(x_batch)\n",
    "        loss = lossfn(mult, y_batch)\n",
    "        loss_mean.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train\", loss, e)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #descente de gradient\n",
    "        with torch.no_grad():\n",
    "            model.weight -= learningRate * model.weight.grad\n",
    "            model.bias -= learningRate * model.bias.grad\n",
    "        model.weight.grad.zero_() #remise a zero des gradients\n",
    "        model.bias.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        mult_test = model(x_test2)\n",
    "        loss_test = lossfn(mult_test, y_test2)\n",
    "        loss_mean_test.append(loss_test.item())\n",
    "        writer.add_scalar(\"Loss/test\", loss_test, e)\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='optim')\n",
    "model = torch.nn.Linear(13, 1).double()\n",
    "lossfn = torch.nn.MSELoss()\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=20, shuffle=True)\n",
    "learningRate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "optim=torch.optim.SGD(params=model.parameters(), lr=learningRate)\n",
    "optim.zero_grad()\n",
    "for e in tqdm(range(epochs)): \n",
    "    for x_batch, y_batch in trainloader:\n",
    "        loss = lossfn(model(x_batch),y_batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        loss_mean.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train\", loss, e)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mult_test = model(x_test2)\n",
    "        loss_test = lossfn(mult_test, y_test2)\n",
    "        loss_mean_test.append(loss_test.item())\n",
    "        writer.add_scalar(\"Loss/test\", loss_test, e)\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='sequential')\n",
    "lin = torch.nn.Linear(13, 5).double()\n",
    "tanh = torch.nn.Tanh().double()\n",
    "lin2 = torch.nn.Linear(5, 1).double()\n",
    "lossfn = torch.nn.MSELoss()\n",
    "model = torch.nn.Sequential(lin, tanh, lin2)\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=20, shuffle=True)\n",
    "learningRate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "optim=torch.optim.SGD(params=model.parameters(), lr=learningRate)\n",
    "optim.zero_grad()\n",
    "for e in tqdm(range(epochs)): \n",
    "    for x_batch, y_batch in trainloader:\n",
    "        loss = lossfn(model(x_batch),y_batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        loss_mean.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train\", loss, e)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mult_test = model(x_test2)\n",
    "        loss_test = lossfn(mult_test, y_test2)\n",
    "        loss_mean_test.append(loss_test.item())\n",
    "        writer.add_scalar(\"Loss/test\", loss_test, e)\n",
    "\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch %d \" % e , \"train MSE: \", np.array(loss_mean).mean(), \"val MSE: \", np.array(loss_mean).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
